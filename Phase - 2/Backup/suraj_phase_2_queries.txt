
USE .PEM file WHILE CREATING THE SECURITY KEY
and then convert it to .ppk file using puttyGen and use that .ppk in putty to perform below putty operations.




1. 
hdfs dfs -mkdir -p /user/hive/warehouse -creating “warehouse” directory inside “hive” directory

2.
hdfs dfs -chmod g+w /user/hive/warehouse - granting write permission to the group

3. For 2003.csv File:

wget https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/HG7NV7/YGU3TD

mv ':persistentId?persistentId=doi:10.7910%2FDVN%2FHG7NV7%2FYGU3TD' 2000.csv.bz2


bzip2 -d 2000.csv.bz2 - unzip

hdfs dfs -put 2000.csv /user/hive/warehouse  - Copying the new file back to HDFS

4.Creation of hive table and load all the data into it 

hive
Create database suraj_flight_info; -creating new database “suraj_flight_info”
Use suraj_flight_info; -Using the new database

CREATE TABLE IF NOT EXISTS suraj2000 (
Year INT,
Month INT,
DayofMonth INT,
DayOfWeek INT,
DepTime STRING,
CRSDepTime STRING,
ArrTime STRING,
CRSArrTime STRING,
UniqueCarrier STRING,
FlightNum INT,
TailNum INT,
ActualElapsedTime INT,
CRSElapsedTime INT,
AirTime INT,
ArrDelay INT,
DepDelay STRING,
Origin STRING,
Dest STRING,
Distance INT,
TaxiIn INT,
TaxiOut STRING,
Cancelled INT,
CancellationCode STRING,
Diverted INT,
CarrierDelay INT,
WeatherDelay INT,
NASDelay INT,
SecurityDelay INT,
LateAircraftDelay INT,
PRIMARY KEY (UniqueCarrier, Origin, Dest) DISABLE NOVALIDATE
)
COMMENT 'Flight Info'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
"seperatorChar" = ","
);

LOAD DATA INPATH  '/user/hive/warehouse/2000.csv' OVERWRITE INTO TABLE suraj2000; - Loading the data to table “suraj2000”



Making a new directory:
[hadoop@ip-172-31-22-108 ~]$ hdfs dfs -mkdir selected_data
[hadoop@ip-172-31-22-108 ~]$ hive
hive> SET hive.cli.print.header=true;
use suraj_flight_info;

5.
CREATE TABLE IF NOT EXISTS suraj_samples (
Year INT,
Month INT,
DayofMonth INT,
DayOfWeek INT,
DepTime STRING,
CRSDepTime STRING,
ArrTime STRING,
CRSArrTime STRING,
UniqueCarrier STRING,
FlightNum INT,
TailNum INT,
ActualElapsedTime INT,
CRSElapsedTime INT,
AirTime INT,
ArrDelay INT,
DepDelay STRING,
Origin STRING,
Dest STRING,
Distance INT,
TaxiIn INT,
TaxiOut STRING,
Cancelled INT,
CancellationCode STRING,
Diverted INT,
CarrierDelay INT,
WeatherDelay INT,
NASDelay INT,
SecurityDelay INT,
LateAircraftDelay INT,
PRIMARY KEY (UniqueCarrier, Origin, Dest) DISABLE NOVALIDATE
)
COMMENT 'Flight Info'
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
"seperatorChar" = ","
);


6. 
INSERT INTO TABLE suraj_samples 
SELECT * FROM suraj2000
DISTRIBUTE BY RAND()
SORT BY RAND()
LIMIT 30000; 

7. 
CREATE TABLE temp AS
SELECT *,
       CASE
           WHEN ArrDelay <= 0 AND DepDelay <= 0 THEN 'N'
           ELSE 'Y'
       END AS Delayed
FROM suraj_samples;

8.
Query:
INSERT OVERWRITE DIRECTORY 'hdfs:///user/hadoop/selected_data/'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
SELECT * FROM temp;
hive> exit;


[hadoop@ip-172-31-22-108 ~]$ hive -e 'SET hive.cli.print.header=true; use suraj_flight_info ; SELECT * FROM  temp LIMIT 0' \ | sed 's/[\]/,/g'> /home/hadoop/header.csv
[hadoop@ip-172-31-22-108 ~]$ hadoop fs -get /user/hadoop/selected_data/* /home/hadoop/data
[hadoop@ip-172-31-22-108 ~]$ cat /home/hadoop/header.csv /home/hadoop/data > /home/hadoop/2000_data.csv


9.
Go to Git Bash app (download it if u dont have) and run:
chmod 400 "C:\\Users\\suraj tamboli\\Downloads\\final_key_for_project.pem" ...(to give permission to .pem file)


10.
Go to Power shell terminal (to get file in our local machine)
scp -i "C:\\Users\\suraj tamboli\\Downloads\\final_key_for_project.pem" hadoop@ec2-3-145-198-202.us-east-2.compute.amazonaws.com:/home/hadoop/2000_data.csv  "C:\\Users\\suraj tamboli\\Downloads\\"

Type Yes
Warning: Permanently added 'ec2-3-134-244-20.us-east-2.compute.amazonaws.com' (ED25519) to the list of known hosts.
2000_data.csv
Your data is downloaded



